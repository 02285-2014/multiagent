\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{float}
\usepackage{latexsym}
\usepackage{marginnote}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[labelfont=bf]{caption}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{listings}
\usepackage{pdflscape}
\usepackage[a4paper]{geometry}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{colortbl}
\parindent=0pt
\frenchspacing
\reversemarginpar

\pagestyle{fancy}

\fancyhead[L]{\slshape\footnotesize June 2, 2014\\ ${}$\\\textsc{Artificial Intelligence and Multi-agent systems}}
\fancyhead[R]{\slshape\footnotesize \textsc{Andreas Kjeldsen (s092638)}\\\textsc{Morten Eskesen (s133304)}\\\textsc{Peter Carlslund (s113998)}}
\fancyfoot[C]{\thepage}

\newcommand{\tab}{\hspace*{2em}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}

\begin{titlepage}
\begin{center}

\includegraphics[scale=2.0]{../GFX/dtu_logo.pdf}\\[1cm]

\textsc{\LARGE Technical University of Denmark}\\[1.5cm]

\textsc{\Large 02285 Artificial Intelligence and Multi-agent Systems}\\[0.5cm]

% Title
\HRule \\[0.4cm]
{\huge \bfseries Mandatory Assignment 3}\\[0.1cm]
\HRule \\[1.5cm]

% Author and supervisor
\large
\emph{Authors:}
\\[10pt]
Andreas Hallberg \textsc{Kjeldsen}\\
\emph{s092638@student.dtu.dk}
\\[10pt]
Morten Chabert \textsc{Eskesen}\\
\emph{s133304@student.dtu.dk}
\\[10pt]
Peter \textsc{Carlslund}\\
\emph{s113998@student.dtu.dk}

\vfill

% Bottom of the page
{\large June 2, 2014}

\end{center}
\end{titlepage}

${}$
\vspace{-.55cm}

\tableofcontents
\clearpage

\section{Introduction}
\marginpar{\tt Andreas \& \\ Morten}
This project was produced while taking the course Artificial Intelligence and Multi-agent systems at the Technical University of Denmark. The project had to be done in groups of 3 to 6 people. For the development of the project, there were 3 iterations, this is the report detailing the third and final iteration.

\subsection{Scenario}
\marginpar{\tt Andreas \& \\ Morten}
There are 28 agents with different roles. They are dropped into an unknown environment. The agents have properties (health, max health, energy, max energy), a visibility range and role defining which specific actions they can do. The environment is a graph consisting of vertices and edges where vertices have a certain value and edges cost a certain amount of energy to traverse. Lastly there is the notion of a score which is the accumulated sum of the zone score for a team during the game.
 
\subsection{Problem Analysis}
\marginpar{\tt Andreas \& \\ Morten}
The scenario of the game introduces a set of problems to overcome.

\subsubsection{Gathering Knowledge}
\marginpar{\tt Andreas \& \\ Morten}
The agents have no prior knowledge of the graph, no knowledge about where the other agents are in relation to each other, nor where the opponent agents are. The agents must be able to share their knowledge and keep track of their opponents.
 
\subsubsection{Working Together}
\marginpar{\tt Andreas \& \\ Morten}
The agents might wish to perform the exact same action, this is not beneficial, therefore the agents should be able to work out whom should do what. Preferably in such a way that the agent able to perform the action with the probable best outcome, is chosen to perform it. 

\subsubsection{Scoring}
\marginpar{\tt Andreas \& \\ Morten}
The agents must coordinate their actions in accordance with an overall strategy that works towards achieving a good score.

\subsubsection{Agent Strategy}
\marginpar{\tt Andreas \& \\ Morten}
A strategy for the agents must be defined. How aggressive should the Saboteur agent be. When should the agents request to be repaired by the Repairer agent. For how long should the agents focus on gathering knowledge and when should they focus on achieving a good score.

\section{Environment}
\marginpar{\tt Andreas}
The environment is randomly generated, that means assumptions about the environment should not be made. Further the agents are placed at random. The agents are allowed to communicate and share their knowledge.

\subsection{Map}
\marginpar{\tt Andreas}
The map of the environment is being represented as an edge weighted graph. Each vertex has a value indicating its score, each edge has a weight indicating the energy cost of crossing the edge.\\
\\
The distance between two connected vertices will be referred to as a \emph{step}. The amount of steps away the agents can perceive, varies from 1 to 3. Whenever an agent perceives a vertex, he will remember the vertex, including the connecting vertices and the edges between them.\\
\\
The vertices has to be probed to obtain information about how valuable the vertex is. Only the \emph{Explorer} agent can probe vertices. If a vertex is not probed, the value of the vertex is set to be 1. The edges has to be surveyed to obtain information about how much it costs to cross them. All agents can survey edges. The costs of crossing an edge is not depending on whether the edge is surveyed or not. Knowing the edge costs gives advantages when calculating paths for the agents to follow, i.e. shorter paths with lower costs.

\subsection{Knowledge}
\marginpar{\tt Andreas}
The agents share all their knowledge, that is, they have a centralized knowledge base. The agents therefore also have the same perception of the environment. The agents share all their new percepts before planning what they should do next. Having a centralized knowledge base, eliminates the need for communicating messages regarding perceptions of the environment, i.e. new vertices, new edges, opponent spotted.

\section{Strategy}
\marginpar{\tt Morten}
At game start the agents have no knowledge about the environment. They do not know where their fellow agents are located nor do they know where the opponent agents are. Therefore in order to be able to strategize properly for the game, the games is split into two phases. The two phases have been named the \emph{Mapping} and \emph{Zone Control Mode}.

\subsection{Mapping}
\marginpar{\tt Andreas}
When the game starts, the first thing the agents focus on is mapping the environment. The Explorer agent focus on probing vertices, looking for unprobed vertices etc. The other agents walk around randomly looking for unsurveyed edges and opponents. The agents will only try to survey the surrounding edges if a specific amount of unsurveyed edges are visible. This avoids spending too much time surveying when only a few unsurveyed edges are near.

\subsection{Zone Control Mode}
\marginpar{\tt Morten}
At step 150 in the game, Zone Control Mode is activated. This is based on the assumption that a great part of the environment is mapped at step 150. Zone control mode is the phase where the agents defend a zone of the map, i.e. a subgraph of the entire graph. In zone control mode the agents only defend this zone and do not care about what the opponents are doing unless they are attacking us or threatening the zone in any way.\\
\\
Zone control mode is activated because the goal of the game is to achieve the highest score. The game grants points based on the zone score for the team. We have several algorithms that finds a high scoring zone in the graph, which we will cover in the section Zone Control. At each 150th step while in zone control mode, a zone recalculation is made, to see if a higher scoring zone can be found or if there is a way to expand the current zone.

\subsection{Reflection}
\marginpar{\tt Morten}
Our strategy for the game is quite simple. When we enter zone control mode our agents are practically standing still with the hope that this would outscore our opponents. This might however not be the case for several reasons.

\begin{itemize}
	\item If the agents haven't mapped, and probed, the highest scoring zone of the map and the opponent controls this zone. We have no plan in place to counter this scenario and will therefore be outscored by the opponent and loose.
	
	\item If the Explorer agents in the first phase haven't probed many nodes due to being disabled then we may end up defending a zone that does not achieve a high score. This again is a scenario we have no plan in place to counter as the Explorer agents will not probe any nodes not in our controlled zone when entering zone control mode.
\end{itemize}

There are other scenarios we could consider but what one ultimately finds is that our strategy is quite naive. Also if the agents try to defend a zone where the opponents are, the agents will fight for this zone until a zone recalculation is made to find another zone. Our strategy would therefore be an obvious thing to improve if more time had allowed it, as we in general do not counter our opponents as much as we should.

\section{Agents}
\marginpar{\tt Andreas}
Each agent has a specific role. Depending on their assigned role, certain properties and abilities are available to the agent. The available agent roles are Explorer, Inspector, Repairer, Saboteur and Sentinel.\\
\\
The agents make use of the \emph{Belief-Desire-Intention} software model. This means that each agent has its own beliefs, its own desires and its own intentions. A belief is an assumption about the environment, though the agents are certain that a vertex will not change position, they are not certain that an opponent agent spotted will not move away. Therefore not all beliefs cannot be determined to be facts.
A desire is something the agent desires to do. The role specific actions for the agents, determine their desires. The Explorer agent desires to probe unprobed vertices, the Saboteur agent desires to sabotage the opponent agent and so forth. The intentions of the agents are immediate intentions like, "I wish to go to vertex v233, on my I will have to pass through vertices v182 and v377".

\subsection{Agent Base}
\marginpar{\tt Andreas}
The agents have some properties, functionality and actions in common. The agents start by interpreting their percepts, thereby gathering new knowledge and intel regarding the opponent team. The agents are able to determine if an agent is nearby based on the shared knowledge of the environment and the opponent positions. The agents are able to find a path from their current vertex to a goal vertex.

\subsection{Explorer Agent}
\marginpar{\tt Andreas}
The Explorer agent is the only agent capable of probing vertices. The Explorer agent prefers its possible actions in the following order: {\tt Recharge} if energy is below $\frac{1}{3}$ of max, {\tt Probe} if standing on unprobed vertex, {\tt Goto} vertex found to be goal vertex, {\tt Survey} edges if enough unsurveyed are found, {\tt Goto \& Probe} nearest unprobed vertex, {\tt Recharge} if energy is not full otherwise {\tt Skip}.\\
\\
During the initial phase of the game, the agent probes as much as possible. During Zone Control mode, the agent will only probe unprobed vertices within the zone the agents are trying to control.

\subsection{Inspector Agent}
\marginpar{\tt Andreas}
The Inspector agent is the only agent capable of inspecting opponents agents. Inspecting an opponent agent reveals the agents role and properties, i.e. health and energy. The Inspector agent prefers its possible actions in the following order: {\tt Recharge} if energy is below 3, {\tt Goto} vertex found to be goal vertex, {\tt Inspect} opponent agent if one is nearby that hasn't already been inspected, {\tt Survey} edges if enough unsurveyed are found, {\tt Goto} random neighbor vertex, {\tt Recharge} if energy is not full otherwise {\tt Skip}.\\
\\
During the initial phase of the game, the agent walks around randomly inspecting nearby opponents and surveying unsurveyed edges. During Zone Control mode, the agent will inspect a nearby opponent if present, the agent will not track down the opponent to inspect them.

\subsection{Repairer Agent}
\marginpar{\tt Andreas \& \\Peter}
The job of the repairer agent, how we solve it and what it does during zone control mode etc.

\subsection{Saboteur Agent}
\marginpar{\tt Andreas \& \\Peter}
The job of the saboteur agent, how we solve it and what it does during zone control mode etc.

\subsection{Sentinel Agent}
\marginpar{\tt Morten}
The job of the sentinel agent depends on phase of the game and whether or not the agent is disabled. The Sentinel agent is able to perform the following actions: {\tt Skip}, {\tt Goto}, {\tt Parry}, {\tt Recharge} and {\tt Survey}.

\subsubsection{Phase 1: Mapping}
In the first phase the Sentinel will help with the mapping by surveying unsurveyed edges. The sentinel has a visibility range of 3, meaning it can survey a lot of edges at once. If there is nothing to survey within its visibility range, the Sentinel will walk around randomly to further help the mapping process.

%CONTINUE HERE!
However being able to survive by itself is important for the Sentinel agent because we value it as one of lesser valued agents of the given five. Therefore we want to burden the repairer agent as less as possible with the repairing of sentinel agents. That is why the sentinel agent will parry or run away from an opponent if an opponent is close when we are in phase 1. There is maximum number of five times it will parry as we do not want it to be "locked" in some position. However there are things that rank higher in the hierachy of assignments for the sentinel. If it has less than 1/4 of energy then it will always recharge and if it receives a goal node it will goto this node. It will receive a goal node when we enter zone control mode. If the sentinel is disabled it will request help from one of the repairer agents.
 
\subsubsection{Phase 2: Zone control mode}
In the second phase the planning for the sentinel agent becomes a lot more simple. It will still always recharge if it has less than 1/4 energy left and go to a goal node if it receives such a goal node. Otherwise the sentinel will parry if an opponent is close to defend the zone and if no opponent is close it will recharge. Note that in this phase there is no maximum amount of times the sentinel can parry it will parry until its energy is less than 1/4 of its total. As in phase 1 the sentinel will request help if it is disabled.

\section{Planning}
\marginpar{\tt Andreas}
Introduction to planning.

\subsection{Planning Center}
\marginpar{\tt Andreas}
Explain that some actions are only to be performed by one agent, that they "bid" on actions etc.

\subsection{Distress Center}
\marginpar{\tt Andreas}
Explain that agents can call for help, that repairers respond and that the distress center works with the planning center.

\section{Zone Control}
\marginpar{\tt Morten}
Zone control is where we defend a zone from our opponents. As mentioned earlier a good zone is a high scoring zone. In this section we will outline and compare the different algorithms we have implemented to find a high scoring zone, where only one is used in the final implementation. Finally we will explain how we guard the zone.

\subsection{Algorithms}
\marginpar{\tt Andreas \& \\ Morten}
There are a lot of approaches to achieving a good zone. One we have discussed a lot was finding a corner and then isolating this corner by making a "wall" in the map. This will cause a lot of nodes to be dominated by us and then it will probably achieve a high score even though our decision to isolate this corner had nothing to do with calculating the score of the nodes. However we have not found a way to identify corners which has lead us to consider other approaches. In the following sections we will describe these approaches and the algorithms we have implemented for this approach.

\subsubsection{Isolated Subgraph}
\marginpar{\tt Morten}
The isolated subgraph is an approach where we pick a high valued node and then we build a zone around this node. This node becomes the center of the zone. The algorithm depends on how many agents we can use to build a zone. It will expand from the center node until we reach a number of nodes higher than the amount of agents. Then it will place the agents such that we dominate all the nodes inside this zone.

\subsubsection{Max Sum Component}
\marginpar{\tt Andreas}
Find connected components, take the one with maximum sum of specific size.

\subsubsection{Simulated Annealing}
\marginpar{\tt Morten}
Simulated annealing is an approach which locates a good approximation to the global optimum for a given function in a large search space. Simulated annealing is a good approach to locating a high scoring zone because the agents only have a certain amount of time to respond to the server and the map is a large search space. With other words the goal is to find an acceptably good solution in a fixed amount of time, rather than the best possible solution. The smart thing about simulated annealing is that it will probalistically choose a solution if it is worse than the current solution found and always choose the new solution if it is better. The reason that it is smart to accept a worse solution is that it will never get stuck at a local optimum.\\
Our implementation of simulated annealing uses the previous algorithm described which finds a connect component of a fixed size and then expands this component for the purpose of getting  a higher scoring zone. We discovered that the algorithm max sum component was really fast so instead of choosing a random node and try to expand around that we could just as well use this algorithm. The algorithm expands the zone by using the rules described for controllng a zone. It picks a random node which is not already dominated by us and is adjacent to one of the nodes we already have an agent placed at. Our implemntation runs until the variable $temperature > 1$ where its initial value is 1000 and each iteration the temperature will "cool" by $0.025$. The function to deem a solution acceptable (if the new solution is not a higher scoring zone) is Euler's number $e$ raised to $(currentScore - newScore) / temperature)$, where $currentScore$ it the score of the current created zone and $newScore$ is the score of the newly created zone.
$$e^{(currentScore - newScore) / temperature}$$
If this function is greater than a (pseudo) random number between 0 and 1 we accept the new solution.

\subsubsection{Comparison}
\marginpar{\tt Morten}
In this section we will compare the algorithm by simulation a game versus only one opponent which will only skip so we can see the outcome of the algorithms in the most optimal setting.
\begin{center}
\begin{tabular}{| c c |}
\hline
Algorithm & Score\\ \hline
Isolated Subgraph & 0 \\
Max Sum Component & 0 \\
Simulated Annealing & 0 \\
\hline
\end{tabular}
\end{center}

\subsection{Guarding The Zone}
\marginpar{\tt Morten}
We guard our zone by having all the agents which are able to parry parry such that they survive when enemies attack them. If opponents are able to intrude our zone we attack them with the hope of killing them and regaining control of our zone. We also guard our zone by repairing all the agents that become disabled as the result of an intrusion or attack on our zone in order not to lose points.

\section{Conclusion}
\marginpar{\tt Andreas \& \\ Morten \& \\ Peter}
Conclude on the problems, state that we solved most of them if not all.

\subsection{Flaws}
\marginpar{\tt Peter}
A shit load of flaws in our solution and hopefully how to remedy them had we had more time!


\end{document}
