\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{tocbibind}
\usepackage{float}
\usepackage{latexsym}
\usepackage{marginnote}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[labelfont=bf]{caption}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{listings}
\usepackage{pdflscape}
\usepackage[a4paper]{geometry}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{fancyref}
\usepackage{hyperref}
\parindent=0pt
\frenchspacing
\reversemarginpar

\makeatletter
\renewcommand\tableofcontents{%
    \@starttoc{toc}%
}
\makeatother

\pagestyle{fancy}

\fancyhead[L]{\slshape\footnotesize June 2, 2014\\ ${}$\\\textsc{Artificial Intelligence and Multi-agent systems}}
\fancyhead[R]{\slshape\footnotesize \textsc{Andreas Kjeldsen (s092638)}\\\textsc{Morten Eskesen (s133304)}\\\textsc{Peter Carlslund (s113998)}}
\fancyfoot[C]{\thepage}

\newcommand{\tab}{\hspace*{2em}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}

\begin{titlepage}
\begin{center}

\includegraphics[scale=2.0]{../GFX/dtu_logo.pdf}\\[1cm]

\textsc{\LARGE Technical University of Denmark}\\[1.5cm]

\textsc{\Large 02285 Artificial Intelligence and Multi-agent Systems}\\[0.5cm]

% Title
\HRule \\[0.4cm]
{\huge \bfseries Mandatory Assignment 3}\\[0.1cm]
\HRule \\[1.5cm]

% Author and supervisor
\large
\emph{Authors:}
\\[10pt]
Andreas Hallberg \textsc{Kjeldsen}\\
\emph{s092638@student.dtu.dk}
\\[10pt]
Morten Chabert \textsc{Eskesen}\\
\emph{s133304@student.dtu.dk}
\\[10pt]
Peter \textsc{Carlslund}\\
\emph{s113998@student.dtu.dk}

\vfill

% Bottom of the page
{\large June 2, 2014}

\end{center}
\end{titlepage}

${}$
\vspace{-.55cm}

\section*{Contents}
\tableofcontents

\section{Introduction}
\marginpar{\tt Andreas \& \\ Morten}
This project was produced while taking the course Artificial Intelligence and Multi-agent systems at the Technical University of Denmark. The project had to be done in groups of 3 to 6 people. For the development of the project, there were 3 iterations, this is the report detailing the third and final iteration. Each section and subsection is annotated with whom contributed to the writing of the section. Sections regarding implementation specific areas are written by the persons who wrote the actual implementation.

\subsection{Scenario}
\marginpar{\tt Andreas \& \\ Morten}
There are 28 agents with different roles. They are dropped into an unknown environment. The agents have properties (health, max health, energy, max energy), a visibility range and role defining which specific actions they can do. The environment is a graph consisting of vertices and edges where vertices have a certain value and edges cost a certain amount of energy to traverse. Lastly there is the notion of a score which is the accumulated sum of the zone score for a team during the game.
 
\subsection{Problem Analysis}
\marginpar{\tt Andreas \& \\ Morten}
The scenario of the game introduces a set of problems to overcome.

\subsubsection{Gathering Knowledge}
\marginpar{\tt Andreas \& \\ Morten}
The agents have no prior knowledge of the graph, no knowledge about where the other agents are in relation to each other, nor where the opponent agents are. The agents must be able to share their knowledge and keep track of their opponents.
 
\subsubsection{Working Together}
\marginpar{\tt Andreas \& \\ Morten}
The agents might wish to perform the exact same action, this is not beneficial, therefore the agents should be able to work out whom should do what. Preferably in such a way that the agent able to perform the action with the probable best outcome, is chosen to perform it. 

\subsubsection{Scoring}
\marginpar{\tt Andreas \& \\ Morten}
The agents must coordinate their actions in accordance with an overall strategy that works towards achieving a good score.

\subsubsection{Agent Strategy}
\marginpar{\tt Andreas \& \\ Morten}
A strategy for the agents must be defined. How aggressive should the Saboteur agent be. When should the agents request to be repaired by the Repairer agent. For how long should the agents focus on gathering knowledge and when should they focus on achieving a good score.

\section{Environment}
\marginpar{\tt Andreas}
The environment is randomly generated, that means assumptions about the environment should not be made. Further the agents are placed at random. The agents are allowed to communicate and share their knowledge.

\subsection{Map}
\marginpar{\tt Andreas}
The map of the environment is being represented as an edge weighted graph. Each vertex has a value indicating its score, each edge has a weight indicating the energy cost of traversing the edge. The agents cannot determine specific coordinates of the vertices, thereby making it hard to determine the direction an agent would go.\\
\\
The distance between two connected vertices will be referred to as a \emph{step}. The amount of steps away the agents can perceive, varies from 1 to 3. Whenever an agent perceives a vertex, he will remember the vertex, including the connecting vertices and the edges between them.\\
\\
The vertices has to be probed to obtain information about how valuable the vertex is. Only the \emph{Explorer} agent can probe vertices. If a vertex is not probed, the value of the vertex is set to be 1. The edges has to be surveyed to obtain information about how much it costs to traverse them. All agents can survey edges. The costs of traversing an edge is not depending on whether the edge is surveyed or not. Knowing the edge costs gives advantages when calculating paths for the agents to follow, i.e. shorter paths with lower costs.

\subsection{Knowledge}
\marginpar{\tt Andreas}
The agents share all their knowledge, that is, they have a centralized knowledge base. The agents therefore also have the same perception of the environment. The agents share all their new percepts before planning what they should do next. Having a centralized knowledge base, eliminates the need for communicating messages regarding perceptions of the environment, i.e. new vertices, new edges, opponent spotted.

\section{Strategy}
\marginpar{\tt Morten}
At game start the agents have no knowledge about the environment. They do not know where their fellow agents are located nor do they know where the opponent agents are. Therefore in order to be able to strategize properly for the game, the games is split into two phases. The two phases have been named the \emph{Mapping} and \emph{Zone Control Mode}.

\subsection{Mapping}
\marginpar{\tt Andreas}
When the game starts, the first thing the agents focus on is mapping the environment. The Explorer agent focus on probing vertices, looking for unprobed vertices etc. The other agents walk around randomly looking for unsurveyed edges and opponents. The agents will only try to survey the surrounding edges if a specific amount of unsurveyed edges are visible. This avoids spending too much time surveying when only a few unsurveyed edges are near.

\subsection{Zone Control Mode}
\marginpar{\tt Andreas \& \\Morten}
At step 150 in the game, Zone Control mode is activated. This is based on the assumption that a great part of the environment is mapped at step 150. Zone Control mode is the phase where the agents defend a zone of the map, i.e. a subgraph of the entire graph. In Zone Control mode the agents only defend this zone and do not care about what the opponents are doing unless they are attacking us or threatening the zone in any way.\\
\\
Zone Control mode is activated because the goal of the game is to achieve the highest score. The game grants points based on the zone score for the team. We have several algorithms that finds a high scoring zone in the graph, which we will cover in the section Zone Control. At each 150th step while in zone control mode, a zone recalculation is made, to see if a higher scoring zone can be found or if there is a way to expand the current zone.

\subsubsection{Path Planning}
During Zone Control mode the agents are to place themselves at specific vertices. Each agent must plan their way to their specific vertex. We have chosen to use Dijkstra's Algorithm for path planning. The path chosen is based on the cost of the edge and the amount of steps required to reach the specified vertex. We prefer paths with lower edge cost but also a path that requires fewer steps. A path could contain a small amount of edges to traverse but they could all be costly, therefore it might be advantageous to take a longer path with lower edge costs to avoid spending too much time recharging. 

\subsection{Reflection}
\marginpar{\tt Morten}
Our strategy for the game is quite simple. When we enter zone control mode our agents are practically standing still with the hope that this would outscore our opponents. This might however not be the case for several reasons.

\begin{itemize}
	\item If the agents haven't mapped, and probed, the highest scoring zone of the map and the opponent controls this zone. We have no plan in place to counter this scenario and will therefore be outscored by the opponent and loose.
	
	\item If the Explorer agents in the first phase haven't probed many nodes due to being disabled then we may end up defending a zone that does not achieve a high score. This again is a scenario we have no plan in place to counter as the Explorer agents will not probe any nodes not in our controlled zone when entering zone control mode.
\end{itemize}

There are other scenarios we could consider but what one ultimately finds is that our strategy is quite naive. Also if the agents try to defend a zone where the opponents are, the agents will fight for this zone until a zone recalculation is made to find another zone. Our strategy would therefore be an obvious thing to improve if more time had allowed it, as we in general do not counter our opponents as much as we should.

\section{Agents}
\marginpar{\tt Andreas}
Each agent has a specific role. Depending on their assigned role, certain properties and abilities are available to the agent. The available agent roles are Explorer, Inspector, Repairer, Saboteur and Sentinel.\\
\\
The agents make use of the \emph{Belief-Desire-Intention} software model. This means that each agent has its own beliefs, its own desires and its own intentions. A belief is an assumption about the environment, though the agents are certain that a vertex will not change position, they are not certain that an opponent agent spotted will not move away. Therefore not all beliefs cannot be determined to be facts.
A desire is something the agent desires to do. The role specific actions for the agents, determine their desires. The Explorer agent desires to probe unprobed vertices, the Saboteur agent desires to sabotage the opponent agent and so forth. The intentions of the agents are immediate intentions like, "I wish to go to vertex v233, on my I will have to pass through vertices v182 and v377".\\
\\
An agent can be disabled, meaning it has 0 health left. A disabled agent is still able to {\tt Goto} a vertex, {\tt Recharge} to get energy and {\tt Skip} its turn. A disabled Repairer agent can also use {\tt Repair}. A disabled agent does not contribute to the zone scoring.\\
\\
Each action available for the agents requires a certain amount of energy. The amount of energy required depends on the action. If an agent tries to perform an action that requires more energy than the agent has available, the action will fail. An agent can get more energy by using the {\tt Recharge} action. If an agent is disabled it can still {\tt Recharge} but the amount of energy recovered is less than if the agent wasn't disabled.

\subsection{Agent Base}
\marginpar{\tt Andreas}
The agents have some properties, functionality and actions in common. The agents start by interpreting their percepts, thereby gathering new knowledge and intel regarding the opponent team. The agents are able to determine if an agent is nearby based on the shared knowledge of the environment and the opponent positions. The agents are able to find a path from their current vertex to a goal vertex. The agents are able to make distress calls, which indicates that they're in need of repairing.\\
\\
In the event that an agent tries to perform an action but receives notification that they do not have enough energy to do so, the agent is forced to perform a {\tt Recharge} before attempting to do the same action again. This avoids the case when an agent has an intention to do something, but not enough energy to do it, which could lead to an agent being stuck.

\subsection{Explorer Agent}
\marginpar{\tt Andreas}
The Explorer agent is the only agent capable of probing vertices. The Explorer agent is able to perform the following actions: {\tt Skip}, {\tt Goto}, {\tt Recharge}, {\tt Survey} and {\tt Probe}.

\subsubsection*{Phase 1: Mapping}
The Explorer agent is a valued agent during the first phase. The Explorer should probe as many unprobed vertices as possible. Due to this, it prefers probing over surveying. If the Explorer is standing on an already probed vertex, it will find the closest unprobed vertex, {\tt Goto} it and {\tt Probe} it. The Explorer has a visibility range of 2, which makes it capable to perceive a good amount of vertices and edges while walking around the environment. The Explorer will try to survey unsurveyed edges if it can see at least 6 unsurveyed edges. The Explorer agent is vital for achieving a good zone score, as unprobed vertices only grants 1 point. Therefore the Explorer agent makes a distress call if its health drops below 40\%. Whenever the Explorer energy drops below $\frac{1}{3}$ it will {\tt Recharge}.

\subsubsection*{Phase 2: Zone Control Mode}
When Zone Control mode is activated, the Explorer will {\tt Goto} a goal node to maintain the zone. While maintaining a zone, the Explorer will stand still, performing {\tt Recharge} if energy is not full otherwise it will {\tt Skip}. If an unprobed vertex is found within the maintained zone, the Explorer will {\Goto} it and {\tt Probe} it to further increase the zone score. If the Explorer becomes disabled, it will make a distress call.

\subsection{Inspector Agent}
\marginpar{\tt Andreas}
The Inspector agent is the only agent capable of inspecting opponents agents. Inspecting an opponent agent reveals the agents role and properties, i.e. health and energy. The Inspector agent is able to perform the following actions: {\tt Skip}, {\tt Goto}, {\tt Recharge}, {\tt Survey} and {\tt Inspect}.

\subsubsection*{Phase 1: Mapping}
During the first phase the Inspector agent will help with the mapping by surveying unsurveyed edges. The Inspector has a limited visibility range of 1 and can therefore not see very far. Due to this, the Inspector can only {\tt Survey} a few edges at a time. The visibility range of the Inspector also limits its ability to spot opponent agents. The Inspector agent will try to {\tt Inspect} an opponent when its within the Inspector visibility range. An opponent will only be inspected once. If no unsurveyed edges have been found and no previously inspected opponent is nearby, the Inspector will {\tt Goto} a random vertex. If the energy of the Inspector drops below 3, it will {\tt Recharge}. If the health of the Inspector drops below 40\% it will make a distress call.

\subsubsection*{Phase 2: Zone Control Mode}
During Zone Control mode, the Inspector will {\tt Goto} a goal vertex and stand still. If an opponent is visible and not previously inspected, the Inspector will {\tt Inspect} it. As in phase 1, if the Inspectors energy drops below 3, it will {\tt Recharge}. While standing still and fully recharged, the Inspector will simply {\tt Skip} its turn. If the Inpsector becomes disabled, it will make a distress call.

\subsection{Repairer Agent}
\marginpar{\tt Andreas \& \\Peter}
The job of the repairer agent, how we solve it and what it does during zone control mode etc.

\subsection{Saboteur Agent}
\marginpar{\tt Andreas \& \\Peter}
The job of the saboteur agent, how we solve it and what it does during zone control mode etc.

\subsection{Sentinel Agent}
\marginpar{\tt Morten}
The job of the Sentinel agent depends on phase of the game and whether or not the agent is disabled. The Sentinel agent is able to perform the following actions: {\tt Skip}, {\tt Goto}, {\tt Parry}, {\tt Recharge} and {\tt Survey}.

\subsubsection*{Phase 1: Mapping}
In the first phase the Sentinel will help with the mapping by surveying unsurveyed edges. The Sentinel has a visibility range of 3, meaning it can survey a lot of edges at once. If there is nothing to survey within its visibility range, the Sentinel will walk around randomly to further help the mapping process.\\
\\
Being able to survive by itself is important for the Sentinel agent because we value it as one of the lesser valued agents. Therefore the Sentinel agent should burden the Repairer agents as little as possible. That is why the Sentinel agent will {\tt Parry} or run away from an opponent if an opponent is close during the Mapping phase. The Sentinel agent will {\tt Parry} a maximum of 5 times in a row, as we wanted to avoid the agent being stuck at some position. However there are things that rank higher in the hierarchy of assignments for the Sentinel. If it has less than $\frac{1}{4}$ of energy then it will always {\tt Recharge} and if it receives a goal vertex it will {\tt Goto} this vertex. The Sentinel will receive a goal vertex when Zone Control mode gets activated. If the Sentinel is disabled it will make a distress call.
 
\subsubsection*{Phase 2: Zone Control Mode}
In the second phase the planning for the Sentinel agent becomes a lot more simple. The Sentinel will still always {\tt Recharge} if it has less than $\frac{1}{4}$ energy left and {\tt Goto} a goal vertex if it receives such a goal vertex. Otherwise the Sentinel will parry if an opponent is close to defend the zone and if no opponent is close it will {\tt Recharge}. Note that in this phase there is no maximum amount of times the Sentinel can {\tt Parry} it will {\tt Parry} until its energy is less than $\frac{1}{4}$ of its total. As in phase 1 the Sentinel will make a distress call if it is disabled.

\section{Planning}
\marginpar{\tt Andreas}
Introduction to planning.

\subsection{Planning Center}
\marginpar{\tt Andreas}
We created a system, we'll be referring to as the \emph{planning center}. The planning center keeps track of the actions the agents intent to perform. It refuses actions that would be a duplicate of an already planned action, while also preferring actions that are more beneficial than already planned actions. A more beneficial plan could be going to an unprobed vertex in as few steps as possible. The agents will only perform their action, when all agents have an action to perform that does not conflict with the others.\\
\\
The agents plan their turn in a sequential order. The planning center therefore acts as a bidding system. The agents bid on the action they wish to perform, using how much the action would cost as their offer. Some actions are 'first come, first serve', these include probing of a specific vertex, surveying from a specific vertex, attacking a specific opponent agent or repairing a specific fellow agent. Other actions are given to the agent with the best offer. Best offer is determined based on the action. For going to and probing an unprobed vertex, the best offer is the lowest amount of steps required to reach the vertex. If an agent is outbid, the agent will have to come up with a new action to perform.

\subsection{Distress Center}
\marginpar{\tt Andreas}
Explain that agents can call for help, that repairers respond and that the distress center works with the planning center.

\section{Zone Control}
\marginpar{\tt Morten}
Zone control is where the agents defend a zone from their opponents. As mentioned earlier a good zone is a high scoring zone. In this section we will outline and compare the different algorithms we have implemented to find a high scoring zone, where only one is used in the final implementation. Finally we will explain how we guard the zone.

\subsection{Algorithms}
\marginpar{\tt Andreas \& \\ Morten}
There are a lot of approaches to achieving a good zone. One we have discussed a lot was finding a corner and then isolating this corner by making a "wall" in the map. This will cause a lot of nodes to be dominated by us and then it will probably achieve a high score even though our decision to isolate this corner had nothing to do with calculating the score of the nodes. However we have not found a way to identify corners which has lead us to consider other approaches. In the following sections we will describe these approaches and the algorithms we have implemented.

\subsubsection{Isolated Subgraph}
\marginpar{\tt Morten}
The isolated subgraph is an approach where we pick a high valued vertex and then build a zone around it. The vertex becomes the center of the zone. The algorithm depends on how many agents we can use to build a zone. It will expand from the center vertex until we reach a number of vertices higher than the amount of agents. Then it will place the agents such that we dominate all the nodes inside this zone.

\subsubsection{Max Sum Component}
\marginpar{\tt Andreas}
Find connected components, take the one with maximum sum of specific size.

\subsubsection{Simulated Annealing}
\marginpar{\tt Morten}
Simulated annealing is an approach which locates a good approximation to the global optimum for a given function in a large search space. Simulated annealing is a good approach to locating a high scoring zone because the agents only have a certain amount of time to respond to the server and the map is a large search space. With other words the goal is to find an acceptably good solution in a fixed amount of time, rather than the best possible solution. The smart thing about simulated annealing is that it will probabilistically choose a solution if it is worse than the current solution found and always choose the new solution if it is better. The reason that it is smart to accept a worse solution is that it will never get stuck at a local optimum.\\
\\
Our implementation of simulated annealing uses the previous algorithm described which finds a connect component of a fixed size and then expands this component for the purpose of getting a higher scoring zone. We discovered that the algorithm max sum component was really fast so instead of choosing a random vertex and try to expand around that, we could just as well use this algorithm. The algorithm expands the zone by using the rules described for controlling a zone. It picks a random vertex which is not already dominated by us and is adjacent to one of the vertices we already have an agent placed at. Our implementation runs until the variable $temperature > 1$ where its initial value is 1000 and each iteration the temperature will "cool" by 2.5\%. The function to deem a solution acceptable (if the new solution is not a higher scoring zone) is Euler's number $e$ raised to $(currentScore - newScore) / temperature$, where $currentScore$ it the score of the current created zone and $newScore$ is the score of the newly created zone.
$$e^\frac{currentScore - newScore}{temperature}$$
If this function is greater than a (pseudo) random number between 0 and 1 we accept the new solution.

\subsubsection{Comparison}
\marginpar{\tt Morten}
We wish to compare the algorithms, to do so we will play several games using the different algorithms. The algorithms will be run on the same map, meaning we choose a set of seeds for the map generation. We focus only on achieving a good score, therefore the opponent team will be limited to a single agent, that will do nothing but {\tt Skip}.

\begin{center}
\begin{tabular}{| c c |}
	\hline
	Algorithm & Score\\ \hline
	Isolated Subgraph & 0 \\
	Max Sum Component & 0 \\
	Simulated Annealing & 0 \\
\hline
\end{tabular}
\end{center}

\subsection{Guarding The Zone}
\marginpar{\tt Morten}
The agents guard their zone by using {\tt Parry} if possible. This increases their chances of surviving if an opponent attacks. If opponents are able to intrude the zone, the agents attack them with the hope of killing them and regaining control of the zone. The agents also guard the zone by repairing the agents making distress calls due to being disabled.

\section{Conclusion}
\marginpar{\tt Andreas \& \\ Morten \& \\ Peter}
Conclude on the problems, state that we solved most of them if not all.

\subsection{Flaws}
\marginpar{\tt Peter}
A shit load of flaws in our solution and hopefully how to remedy them had we had more time!

\begin{thebibliography}{9}

\bibitem{arti}
 	Stuart J. Russel, Peter Norvig:
 	\emph{Artificial Intelligence: A Modern Approach}.
	3rd Edition, 	
	Pearson,
	2010.
	
\bibitem{clrs}
 	Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein:
	\emph{Introduction To Algorithms}.
	3rd Edition, 	
	MIT Press,
	2009.
	
\bibitem{algs4}
 	Robert Sedgewick, Kevin Wayne:
	\emph{Algorithms}.
	4th Edition, 	
	Addison Wesley,
	2011.

\end{thebibliography}


\end{document}
