\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{float}
\usepackage{latexsym}
\usepackage{marginnote}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[labelfont=bf]{caption}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{listings}
\usepackage{pdflscape}
\usepackage[a4paper]{geometry}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{colortbl}
\parindent=0pt
\frenchspacing
\reversemarginpar

\pagestyle{fancy}

\fancyhead[L]{\slshape\footnotesize June 2, 2014\\ ${}$\\\textsc{Artificial Intelligence and Multi-agent systems}}
\fancyhead[R]{\slshape\footnotesize \textsc{Andreas Kjeldsen (s092638)}\\\textsc{Morten Eskesen (s133304)}\\\textsc{Peter Carlslund (s113998)}}
\fancyfoot[C]{\thepage}

\newcommand{\tab}{\hspace*{2em}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}

\begin{titlepage}
\begin{center}

\includegraphics[scale=2.0]{../GFX/dtu_logo.pdf}\\[1cm]

\textsc{\LARGE Technical University of Denmark}\\[1.5cm]

\textsc{\Large 02285 Artificial Intelligence and Multi-agent Systems}\\[0.5cm]

% Title
\HRule \\[0.4cm]
{\huge \bfseries Mandatory Assignment 3}\\[0.1cm]
\HRule \\[1.5cm]

% Author and supervisor
\large
\emph{Authors:}
\\[10pt]
Andreas Hallberg \textsc{Kjeldsen}\\
\emph{s092638@student.dtu.dk}
\\[10pt]
Morten Chabert \textsc{Eskesen}\\
\emph{s133304@student.dtu.dk}
\\[10pt]
Peter \textsc{Carlslund}\\
\emph{s113998@student.dtu.dk}

\vfill

% Bottom of the page
{\large June 2, 2014}

\end{center}
\end{titlepage}

${}$
\vspace{-.55cm}

\tableofcontents
\clearpage

\section{Introduction}
\marginpar{\tt Andreas \& \\ Morten}
This project is produced while taking the course Artificial Intelligence and Multi-agent systems at the Technical University of Denmark.

\subsection{Scenario}
\marginpar{\tt Andreas \& \\ Morten}
The scenario is that we have 28 agents with different roles and they are dropped into an unknown environment. The agents have health, a certain amount of energy, a visibility range and specific actions they can do. The environment is a graph consisting of nodes and edges where nodes have a certain value and edges cost a certain amount of energy to traverse. Lastly there is the notion of a score which is the sum of all the nodes a team dominates.
 
\subsection{Problems}
\marginpar{\tt Andreas \& \\ Morten}
There are a lot of problems we have to overcome with a scenario like this. We have no prior knowledge of the graph, where the opponent agents are or where our agents are in relation to eachother in the map. We will describe problems that applies to all agents and problems that are agent-specific.
 
\subsubsection{Common problems}
\marginpar{\tt Andreas \& \\ Morten}
We have to gain more knowledge of the graph in order to have a map of the environment. But this map should be shared among all of the agents and the agents should know where the other agents are in this internal map which brings a problem of the agents should have some shared knowledge and they should be able to communicate with each other to coordinate their actions. Lastly there is the problem of an overall strategy that should be shared by all the agents and it should achieve a good score according to the rules of the game.
 
\subsubsection{Agent-specific problems}
\marginpar{\tt Andreas \& \\ Morten}
There is some agent-specific problems with the repairer agent and the saboteur agent as their primary actions are dependent on other agents. The saboteur's primary assignment is attacking the opponent's agents and the repairer needs to repair our agents when they are disabled.

\section{Environment}
\marginpar{\tt Andreas}
The environment is randomly generated, that means assumptions about the environment should not be made. Further the agents are placed at random. The agents are allowed to communicate and share their knowledge.

\subsection{Map}
\marginpar{\tt Andreas}
The map of the environment is being represented as an edge weighted graph. Each vertex has a value indicating its score, each edge has a weight indicating the energy cost of crossing the edge.\\
\\
The distance between two connected vertices will be referred to as a \emph{step}. The amount of steps away the agents can perceive, varies from 1 to 3. Whenever an agent perceives a vertex, he will remember the vertex, including the connecting vertices and the edges between them.\\
\\
The vertices has to be probed to obtain information about how valuable the vertex is. Only the \emph{Explorer} agent can probe vertices. If a vertex is not probed, the value of the vertex is set to be 1. The edges has to be surveyed to obtain information about how much it costs to cross them. All agents can survey edges. The costs of crossing an edge is not depending on whether the edge is surveyed or not. Knowing the edge costs gives advantages when calculating paths for the agents to follow, i.e. shorter paths with lower costs.

\subsection{Knowledge}
\marginpar{\tt Andreas}
The agents share all their knowledge, that is, they have a centralized knowledge base. The agents therefore also have the same perception of the environment. The agents share all their new percepts before planning what they should do next. Having a centralized knowledge base, eliminates the need for communicating messages regarding perceptions of the environment, i.e. new vertices, new edges, opponent spotted.

\section{Strategy}
\marginpar{\tt Morten}
When this game starts we have no knowledge about the environment we are in, where the opponent agents or even where our agents are in relation to each other with the exception that they may be able to see each other or opponent agents. Therefore in order to be able to strategize properly for this game we view the game as having two phases. The two phases have been dubbed mapping and zone control mode which we will explain further in this section.

\subsection{Mapping}
\marginpar{\tt Andreas}
When the game starts, the first thing the agents focus on is mapping the environment. The Explorer agent focus on probing vertices, looking for unprobed vertices etc. The other agents walk around randomly looking for unsurveyed edges and opponents. The agents will only try to survey the surrounding edges if a specific amount of unsurveyed edges are visible. This avoids spending too much time surveying when only a few unsurveyed edges are near.

\subsection{Zone Control Mode}
\marginpar{\tt Morten}
When we have mapped a sufficiently amount of the map we go into zone control mode. We have decided that at step 150 of the game we enter zone control mode. Zone control mode is the phase where we defend a zone of the map, i.e. a subgraph of the entire graph. In zone control mode the agents only defend this zone and do not care about what the opponent is doing unless the opponent is attacking us or threatening the zone in any way. We enter the zone control mode because we want to acheive a good score and thereby outscore our opponents. The game grants points based on various factors. One of them is the zone score. Score is defined by us as the zone score, therefore the notion of a good score is a high zone score. We have several algorithms that finds a high scoring zone in the graph, which we will cover in the section Zone Control. At each 150th step while in zone control mode we recalculate to find a high scoring zone to see if we can expand the current zone or find another zone and thereby acheive an even higher score.

\subsection{Reflection}
\marginpar{\tt Morten}
Our strategy for this game is quite simple. When we enter zone control mode our agents are practically standing still with the hope that this would outscore our opponents. This might however not be the case for several reasons.
\begin{itemize}
\item If we haven't mapped (probed) the highest scoring zone of the map and the opponent controls this zone. We have no plan in place to counter this scenario and will therefore be outscored by the opponent and loose.
\item If our Explorer agents in the first phase haven't probed many nodes due to being disabled then we may end up defending a zone that does not acheive a high score. This again is a scenario we have no plan in place to counter as the Explorer agents will not probe any nodes not in our controlled zone when entering zone control mode.
\end{itemize}
There are many other scenarios we could consider but what one ultimately finds is that our strategy is quite naive. Also if we try to defend a zone where the opponents are we will fight for this zone until we recalculate to find a zone. Our strategy would therefore be the object of a modification if more time had allowed it as we in general do not counter our opponents as much as we should.

\section{Agents}
\marginpar{\tt Andreas}
Each agent has a specific role. Depending on their assigned role, certain properties and abilities are available to the agent. The available agent roles are Explorer, Inspector, Repairer, Saboteur and Sentinel.\\
\\
The agents make use of the \emph{Beliefs, Intentions, Desires} schema. This means that each agent has its own beliefs, its own intentions and its own desires. A belief is an assumption about the environment, though the agents are certain that a vertex will not change position, they are not certain that an opponent agent spotted will not move away. Therefore not all beliefs cannot be determined to be facts. An intention is something the agent intents to do. The role specific actions for the agents, determine their intentions. The Explorer agent intents to probe unprobed vertices, the Saboteur agent intents to sabotage the opponent agents and so forth. The desires of the agents are immediate desires like, "I wish to go to vertex v233".

\subsection{Agent Base}
\marginpar{\tt Andreas}
The agents have some properties, functionality and actions in common. The agents start by interpreting their percepts, thereby gathering new knowledge and intel regarding the opponent team. The agents are able to determine if an agent is nearby based on the shared knowledge of the environment and the opponent positions. The agents are able to find a path from their current vertex to a goal vertex.

\subsection{Explorer Agent}
\marginpar{\tt Andreas}
The Explorer agent is the only agent capable of probing vertices. The Explorer agent prefers its possible actions in the following order: {\tt Recharge} if energy is below $\frac{1}{3}$ of max, {\tt Probe} if standing on unprobed vertex, {\tt Goto} vertex found to be goal vertex, {\tt Survey} edges if enough unsurveyed are found, {\tt Goto \& Probe} nearest unprobed vertex, {\tt Recharge} if energy is not full otherwise {\tt Skip}.\\
\\
During the initial phase of the game, the agent probes as much as possible. During Zone Control mode, the agent will only probe unprobed vertices within the zone the agents are trying to control.

\subsection{Inspector Agent}
\marginpar{\tt Andreas}
The Inspector agent is the only agent capable of inspecting opponents agents. Inspecting an opponent agent reveals the agents role and properties, i.e. health and energy. The Inspector agent prefers its possible actions in the following order: {\tt Recharge} if energy is below 3, {\tt Goto} vertex found to be goal vertex, {\tt Inspect} opponent agent if one is nearby that hasn't already been inspected, {\tt Survey} edges if enough unsurveyed are found, {\tt Goto} random neighbor vertex, {\tt Recharge} if energy is not full otherwise {\tt Skip}.\\
\\
During the initial phase of the game, the agent walks around randomly inspecting nearby opponents and surveying unsurveyed edges. During Zone Control mode, the agent will inspect a nearby opponent if present, the agent will not track down the opponent to inspect them.

\subsection{Repairer Agent}
\marginpar{\tt Andreas \& \\Peter}
The job of the repairer agent, how we solve it and what it does during zone control mode etc.

\subsection{Saboteur Agent}
\marginpar{\tt Andreas \& \\Peter}
The job of the saboteur agent, how we solve it and what it does during zone control mode etc.

\subsection{Sentinel Agent}
\marginpar{\tt Morten}
The job of the sentinel agent depends on which phase of the game we are in and of course if it is disabled or not. The sentinel agent is able to perform the following actions: skip, goto, parry, recharge and survey.
\subsubsection{Phase 1: Mapping}
In the first phase the sentinel will help with the mapping by surveying so we can get a better picture of how the graph we are dropped into is connected. This helps us when we have to navigate in the map, especially with a visibility range of 3 as the sentinel agent has. If there is nothing to survey within its visibility range it will walk around to furhter help the mapping process. However being able to survive by itself is important for the sentinel agent because we value it as one of lesser valued agents of the given five. Therefore we want to burden the repairer agent as less as possible with the repairing of sentinel agents. That is why the sentinel agent will parry or run away from an opponent if an opponent is close when we are in phase 1. There is maximum number of five times it will parry as we do not want it to be "locked" in some position. However there are things that rank higher in the hierachy of assignments for the sentinel. If it has less than 1/4 of energy then it will always recharge and if it receives a goal node it will goto this node. It will receive a goal node when we enter zone control mode. If the sentinel is disabled it will request help from one of the repairer agents.
 
\subsubsection{Phase 2: Zone control mode}
In the second phase the planning for the sentinel agent becomes a lot more simple. It will still always recharge if it has less than 1/4 energy left and go to a goal node if it receives such a goal node. Otherwise the sentinel will parry if an opponent is close to defend the zone and if no opponent is close it will recharge. Note that in this phase there is no maximum amount of times the sentinel can parry it will parry until its energy is less than 1/4 of its total. As in phase 1 the sentinel will request help if it is disabled.

\section{Planning}
\marginpar{\tt Andreas}
Introduction to planning.

\subsection{Planning Center}
\marginpar{\tt Andreas}
Explain that some actions are only to be performed by one agent, that they "bid" on actions etc.

\subsection{Distress Center}
\marginpar{\tt Andreas}
Explain that agents can call for help, that repairers respond and that the distress center works with the planning center.

\section{Zone Control}
\marginpar{\tt Morten}
Zone control is where we defend a zone from our opponents. As mentioned earlier a good zone is a high scoring zone. In this section we will outline and compare the different algorithms we have implemented to find a high scoring zone, where only one is used in the final implementation. Finally we will explain how we guard the zone.

\subsection{Algorithms}
\marginpar{\tt Andreas \& \\ Morten}
There are a lot of approaches to achieving a good zone. One we have discussed a lot was finding a corner and then isolating this corner by making a "wall" in the map. This will cause a lot of nodes to be dominated by us and then it will probably achieve a high score even though our decision to isolate this corner had nothing to do with calculating the score of the nodes. However we have not found a way to identify corners which has lead us to consider other approaches. In the following sections we will describe these approaches and the algorithms we have implemented for this approach.

\subsubsection{Isolated Subgraph}
\marginpar{\tt Morten}
The isolated subgraph is an approach where we pick a high valued node and then we build a zone around this node. This node becomes the center of the zone. The algorithm depends on how many agents we can use to build a zone. It will expand from the center node until we reach a number of nodes higher than the amount of agents. Then it will place the agents such that we dominate all the nodes inside this zone.

\subsubsection{Max Sum Component}
\marginpar{\tt Andreas}
Find connected components, take the one with maximum sum of specific size.

\subsubsection{Simulated Annealing}
\marginpar{\tt Morten}
Simulated annealing is an approach which locates a good approximation to the global optimum for a given function in a large search space. Simulated annealing is a good approach to locating a high scoring zone because the agents only have a certain amount of time to respond to the server and the map is a large search space. With other words the goal is to find an acceptably good solution in a fixed amount of time, rather than the best possible solution. The smart thing about simulated annealing is that it will probalistically choose a solution if it is worse than the current solution found and always choose the new solution if it is better. The reason that it is smart to accept a worse solution is that it will never get stuck at a local optimum.\\
Our implementation of simulated annealing uses the previous algorithm described which finds a connect component of a fixed size and then expands this component for the purpose of getting  a higher scoring zone. We discovered that the algorithm max sum component was really fast so instead of choosing a random node and try to expand around that we could just as well use this algorithm. The algorithm expands the zone by using the rules described for controllng a zone. It picks a random node which is not already dominated by us and is adjacent to one of the nodes we already have an agent placed at. Our implemntation runs until the variable $temperature > 1$ where its initial value is 1000 and each iteration the temperature will "cool" by $0.025$. The function to deem a solution acceptable (if the new solution is not a higher scoring zone) is Euler's number $e$ raised to $(currentScore - newScore) / temperature)$, where $currentScore$ it the score of the current created zone and $newScore$ is the score of the newly created zone.
$$e^{(currentScore - newScore) / temperature}$$
If this function is greater than a (pseudo) random number between 0 and 1 we accept the new solution.

\subsubsection{Comparison}
\marginpar{\tt Morten}
In this section we will compare the algorithm by simulation a game versus only one opponent which will only skip so we can see the outcome of the algorithms in the most optimal setting.
\begin{center}
\begin{tabular}{| c c |}
\hline
Algorithm & Score\\ \hline
Isolated Subgraph & 0 \\
Max Sum Component & 0 \\
Simulated Annealing & 0 \\
\hline
\end{tabular}
\end{center}

\subsection{Guarding The Zone}
\marginpar{\tt Morten}
We guard our zone by having all the agents which are able to parry parry such that they survive when enemies attack them. If opponents are able to intrude our zone we attack them with the hope of killing them and regaining control of our zone. We also guard our zone by repairing all the agents that become disabled as the result of an intrusion or attack on our zone in order not to lose points.

\section{Conclusion}
\marginpar{\tt Andreas \& \\ Morten \& \\ Peter}
Conclude on the problems, state that we solved most of them if not all.

\subsection{Flaws}
\marginpar{\tt Peter}
A shit load of flaws in our solution and hopefully how to remedy them had we had more time!


\end{document}
